\section{Evaluation}
\label{sec:evaluation}




We implement our concurrent sketches and evaluate their
performance.
Our implementations are based on the
code in DataSketches~\cite{sketchesLibrary}, which is a Java
open source library of stochastic streaming algorithms.
%We implemented our both concurrent sketches inside
%DataSketches~\cite{}, which is a Java open source library
%of stochastic streaming algorithms.
In both cases (quantiles and theta), we build our concurrent
sketch on top of the sequential sketch implementation, which was
already very optimized in the library.

\subsection{Implementation}
\label{sub:imp}

To avoid false sharing, unnecessary cash misses, and memory
flushes we let each worker thread to locally maintain its private
part of the sketch.
One way to achieve this is by using thread local memory.
However, we found that accessing thread local memory per
every operation decreases the update throughput by $30\%$.
Therefore, we chose a different approach.
Given a shared sketch, instead of calling its methods
directly, every worker thread first wraps it with a context
(sometime called handler) that implements the same interface as
the sketch, and then access the sketch only through its context.
The context is stored in the memory that is close to the core
the thread runs on, and it synchronizes with the shared sketch through the synchronization variables.

  

\subsection{Experiment Setup}
\label{sub:setup}

The experiments run on a dedicated machine with four Intel
Xeon E5-4650 processors, each with $8$ cores, for a total of
$32$ threads (with hyper-threading disabled).
For the experiments we use microbenchmarks, and consider two
representative workloads: (1) an \emph{update-only} workload in
which a sketch is built from a stream, and (2) a \emph{mixed}
workload in which there is a single reader that continuously reads
from the sketch, while the other threads build it.
We run every experiment for 30 seconds. 
Our baselines are the the sketches' sequential implementations
given in the DataSketches library that we wrap with a read-write lock
to allow concurrency.
We also run the sequential sketch with a single update-only thread as a point of reference to the implementations.

\subsection{Theta sketch}
\label{sub:thetaExp}

Figure~\ref{fig:ConccurentTheta} compares the scalability
of our concurrent sketch and lock-based sketch in an update-only workload.
As expected, the lock-based sketch does not scale, and
in fact it performs worse when accessed concurrently by many
threads.
However, our sketch achieves almost linear scalability.
In this graph we show results with the following parameters:
Every local buffer contains 16 items, while the size of the
shared sketch is 4096 items.
It is important to note the we also tried smaller local buffers,
and shorter runs and the results were the same.

\begin{figure}[h]
  \centering
  \includegraphics*[width=3.1in]{images/concurrentThetaGraph}
  \caption{Theta sketch update-only workload}
   \label{fig:ConccurentTheta}
\end{figure}




We also measured the reader
throughput  in a mixed workload.
The reader in the concurrent implementation is able to run 380
millions/sec queries independently of the number of concurrent
writers. This high and stable throughput is due to reading a single atomic variable without interfering with the propagating thread.
The lock-based sketch is only able to run 0.2 millions/sec in case of one concurrent writer. This is due to the high contention with the writer on the lock in addition to the cost of the fence to acquire the lock.
The performance degrades to 0.0006 millions/sec queries when the
number of writers grows to 28.
We also measured the updates throughput in the present of one
reader, but we omit this graph since it looks exactly the same as
the one in Figure~\ref{fig:ConccurentTheta}.

\remove{
\begin{figure}[h]
  \centering
  \includegraphics*[width=4in]{images/concurrentThetaReads}
  \caption{}
   \label{fig:ConccurentThetaReads}
\end{figure}
}



\subsection{Quantiles sketch}
\label{sub:quantilesExp}

Figure~\ref{fig:ConccurentQuantilesUpdate}  compares the
throughput of our concurrent Quantiles sketch to the lock-based sketch.
First, note that the baseline does not scale, and moreover, it
achieves best result with a single updating thread due to the contention on the lock.
Our sketch, scales well when every thread
has enough local levels.
It clearly follows from the graph that in order for the
background thread to support more worker threads we have to
increase the number local levels.
O local levels are enough for 2 threads, 2 local levels are good
for 12 threads, and for perfect scalability on our machine in
update-only workload we need 4 local levels\footnote{counting the number of local levels below the level that is propagated to the shared buffer}.

\begin{figure}[h]
  \centering
  \includegraphics*[width=3.1in]{images/QuantilesUpdate}
  \caption{Quantile sketch update-only workload}
   \label{fig:ConccurentQuantilesUpdate}
\end{figure}

Figure~\ref{fig:ConccurentQuantilesReader} considers a
mixed workload to check how a single reader impacts the update
throughput.
Since the reader in this case continuously reads the atomic
bitPattern variable, and thus forces the background thread to
flush its local memory more often, we see that the background
thread indeed works harder.
Instead of 4 levels in the update-only workload, we now need 5
local levels for perfect scalability, and we can also see that
0 and 2 local levels allow the background thread to support less
worker threads than in the update-only workload.


\begin{figure}[h]
  \centering
  \includegraphics*[width=3.1in]{images/QuantilesMixed}
  \caption{Quantile sketch mixed workload}
   \label{fig:ConccurentQuantilesReader}
\end{figure}
