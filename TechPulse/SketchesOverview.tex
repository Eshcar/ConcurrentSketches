\section{Sketches API and Correctness}
\label{sec:api}


% API 
A sketch $S$ supports three main API methods -- 
\begin{description}
\item[$S$.update($a_i$)] processes stream element $a_i$; and 
\item[$S$.query(arg)] returns the function estimated by the sketch, for example, the number of unique elements; 
 takes an optional argument, e.g., the requested quantile.
 \item[$S$.union($S'$)] merges sketches $S$ and $S'$ into $S$; that is, if $S$ initially represents stream A and $S'$ 
 represents stream $A'$, then after this call, $S$ represents the union of the two streams. 
 Sketches that support this operator are called \emph{mergeable}.
\end{description}

% Sequential specification
Today, sketches are used sequentially; the entire stream $A = a_1, a_2, \dots a_n$ is processed 
and only then the sketch is queried. $S$.query(arg) then returns an estimate of the desired function 
on the entire stream. 
Sketches use randomization and their accuracy is defined in one of two ways:
\begin{enumerate}
\item An \emph{unbiased} sketch provides an estimate  $\hat{e}$ whose expectation is the correct quantity $e$ 
(for example, the number of unique elements in the stream in a $\Theta$ sketch).  
\remove{
The \emph{Relative Standard Error (RSE)} is then derived from the variance $\sigma^2(\hat{e})$  of the estimate
as 
\[ \sqrt {\frac{\sigma^2(\hat{e})}{e^2}} \]
For example, a $\Theta$ sketch consisting of  $K$ samples provides an unbiased approximation $\hat{u}$ of the 
number $u$ of unique elements in the stream with an RSE of $1/\sqrt{K-1}$.
}
\item A \emph{probably approximately correct} sketch provides a result that estimates the correct result
within some error bound $\epsilon$ with a failure probability bounded by some parameter $\delta$.  
%For example, a quantiles sketch approximates the $\phi$ quantile of a stream with  $n$ elements 
%by returning an element with rank between $(\phi-\epsilon)n$ and  $(\phi+\epsilon)n$ with 
%probability at least $1-\delta$.

\end{enumerate} 

% Relaxed semantics
 The correctness criterion for our concurrent sketches is a flavor of 
 \emph{relaxed consistency} due to Henzinger et al.~\cite{Henzinger2013}    
 specifically, a restricted form of their   \emph{out-of-order} relaxation 
 that allows operations to ``overtake'' some of the updates that precede them.  
 For example, a query may return a result that reflects all but a bounded number of the updates
 that precede it. It also allows bounded re-ordering of updates.
%While relaxed semantics were previously used for traditional data structures like stacks~\cite{Henzinger} and priority queues~\cite{alistarh}, 
We believe that relaxed semantics are a natural fit for data sketches
as they are inherently approximate and typically summarize streams that  arise from multiple real-world sources  
and are collected over a network with variable delays. Thus, even if the sketch ensures strict semantics, 
queries might miss some real-world events that occur before them.
%, which is not qualitatively different than the  relaxed sketch. Second, sketches are inherently approximate, even when they are strict. 
Relaxing their semantics therefore makes sense, as long as it does not excessively increase the expected error. 
% correctness roadmap: derandomize,  strong serializability, Adversary model,
With that, proving the concurrent sketches' correctness and analyzing the sketches' error bounds
are out of the scope of this paper.

\remove{ 

\noindent Note that step 3 is the only step that we are required
to prove.


TODO: figure?

TODO: Think about a general scheme that captures both our
algorithm and we can prove is strong linearizable.

\textbf{Comment: I do not know how to bound the error in the
worst case since r-relaxed runs change the summaries internal structure
and thus can potentially add more error than just missing k
values.}



\subsection{General implementation approach}
\label{sub:approach}

% Goals and challenges
Our goal is to build data sketches that (1) allow queries to be processed while the sketch is being built; and (2) 
support parallel construction of the sketch via multiple threads.
The challenge in achieving (1) is that a sketch update is not always atomic, and intermediate 
states of the sketch may be bogus, leading to gross estimation errors. 
For this reason, some applications that use sketches synchronize all access to them, which, as we show below, is detrimental to performance (in our experiment, reducing throughput threefold). 
The challenge with (2) is again, the need to support concurrent queries. 
Although data sketches are naturally amenable to parallel construction via separate threads (or processes) that each summarize a substream followed by a union operator that merges all the substream sketches, this approach 
does not allow queries to be processed in real-time before the sketch is  merged. 




%data structure
While sketches are usually not defined and treated as such, we
prefer to look on sketches as probabilistic append-only data
structures.
Every sketch has an API to append data item to the summary and
an API to read statistics on data items appended so far. 
In Section~\ref{sec:model} we formally define sketches
sequential specification, and a variation of a linearizabilty
property that takes the allowed error into account.
%a slightly weaker linearizabilty
%property that allows us to deal with infinite streams of data.
%
Now once we consider sketches as data structures with sequential
specifications, and we define how a sketch should behave when
accessed concurrently by many threads, we can talk about
concurrent sketches.

%The price of synchronization
However, in contrast to traditional concurrent data-structures,(
e.g., skiplists and B-trees) sketches' algorithms require very
little computation.
So, we should be very careful with the the cost we add
for synchronization. 
For example, as mentioned above, our measurements show that
adding even a single fence per operation in a sequential
execution decreases the throughput by a factor of 3.
Therefore, in order to get a scalable implementation we clearly
have to use as little synchronization as possible.

%Our approach - localirty
Our approach is strongly based on locality.
The common idea for all sketches is to make as much local work
as possible and use very light synchronization to merge it. 
We let threads process data items locally and only once in a
while we propagate their local state to a dedicated background
thread that merge local states with the shared one.
Thanks to the sketches semantics and structure we are able to do
it with a little extra memory cost and sometimes without
increasing the error at all.
Since sketches algorithms use randomized mechanisms to filter
data items, it is very nature to do it in parallel and sometimes
merge the results.
Note that by giving each thread to work locally and using a
dedicated thread for propagation, we also exploit better the
cache locality and pay less during fences. 

% \subsection{General Scheme}
% \label{sub:generalScheme}
% 
% 
% 
% 
% Given a sketch $sk$ with a sequential implementation $S$ such
% that every run of $S$ satisfies $sk$'s sequential specification,
% we give a general scheme to create a concurrent version of
% $S$ that satisfies \emph{r-perforated linearization}.
% First, extend $S$ to support concurrent reads, and denote this
% implementation by $S'$.
% Then, a generic structure that includes (1) a shared
% instance of $S'$; and (2) local data structures $L_1,.. L_n$
% that buffer operations to be added to $S'$.
% To satisfy \emph{r-perforated linearization} ensure that ($A_1$)
% all buffer contain no more than k operations together; and
% ($A_2$) the propagation from each $L_i$ to $S'$ is equivalent to
% running the sequence of operations masked by $L_i$ on $S'$.
% In the following sections we present two concurrent algorithms
% using the above structure and prove that they satisfy conditions
% $A_1$ and $A_2$.
% 
% TODO:
% 
% Axioms
% 
% figure
%  proof
% 
% 
}